<style >
    body{
      margin: 20 75;
    }
</style>

<!DOCTYPE html>
<html>
    <head lang="en">
        <meta charset="utf-8">
        <title>Ai overrated</title>
        
    </head>
    <body>
        <article>
            <h1>Why AI is not replacing nobody anytime soon even if it was better</h1>

            <p>AI has made a lot of noise especially since ChatGPT4 and Midjourney. AI is new buzzword after machine learning and crypto. But interest rate came in the way and VC money is getting shy. Developing AI has always been a game of big actors.</p>
    
            <p>
                Before that LLM were mostly used to create characters that could give company to lonely young people well versed into tech.
                Professionally though, these technologies did not give someone the power to be paid for something he was unable to do before.
                Except maybe for hackers who seem to be the only one to truly exploit the power of AI. Some have been able to take the appearance of real people well positioned in a bank to push for the transfer of million of dollars live in a call conference using deep fakes.
                And still, it was only after mastering the dark art of social engineering needed to get the contact of the participants and gather crucial details to convince everybody around the table.
        
            </p>
            <p>
                In this article though I will focus on enterprise applications of LLM such as chatgpt and give my opinion to the new never-ending question : "will AI replace our jobs?".
                If you are scared about being replaced by something like chatgpt, ask yourself : "why I got hired ?". Not how, but why :    
            </p>
            <h3> Reliability and Responsability</h3>
    
            <p>Thats all. End of article.
                I am joking but maybe just from the title of this very first paragraph, you get where i am getting into.
                Trust and responsability is the very first thing an employer is willing to concede when hiring someone. "Is this person enough trustworthy for me to delegate?"
                Skills is just meant to filter people that are not qualified for the job because they won't be able to deliver quiclky enough to be profitable for the company.
                And if you think that everybody needs to demonstrate proficiency at a job before being hired, you will either be surrounded by geniuses at everyjob or companies will run out of money to spot geniuses from good liars.
            </p>
            <p strong>The first question is then, is LLM trustworthy enough ?
            </p>
            <p> Everybody who tried chatgpt has experienced not only what appears as non-sense (called hallucianations in the AI field) but as well opposite answers.
                It has no idea about its own correctness, let alone uncertainty.
                We all have been around someone who continuelsy talks prefering bullshit to silence. But worse than this friend, who is at least responsible of his claims, the AI is not. And most importantly, it does not have the slightest idea of any self meta cognition. it seems to know what it does not know only when you tell him so. And then, enters again into this loop.
                <ul>
                    <li>Make strong claims</li>
                    <li>Seems to get what is wrong after you correct it</li>
                    <li>Apologize and make again half true claims or sometimes merely repeat the first output</li>
                </ul>
            </p>
            <p>At no point, it will just tell you : "look man, what you are asking is a little of a stretch for me here. I am not supposed to know everything and you better focus on something else until I can come up with better solutions." That is extremely hard to do even for humans, but humans know what they know. But AI does not because it has no capacity to evaluate its own capabilities itself. LLM at least was not built like this. They do not know boundaries, they have been trained to output the next most convincing word. And until then, we are at peace. </p>

            <h3> Downfall of internet and self-reliance</h3>
            <p>
                Using chatgpt is so easy that anybody who is able to send a text message can generate outputs.
            </p>
            <p>And while these outputs are quiet impressive given the youth of the technology, you cannot talk about "results".</p>
            <p> The population is divided into 2 kind of users : those who can falsify it and those who dont. 
                The first have a scientific approach and can take advantage of its outputs and turning them into results. 
                The other one consume the outputs as they would consume anything else. 
                The former produce something valuable, the latter enters into a new illusion of knowledge, more uncertain than their own.</p>
            <p> LLM have to improve and keep being trained, but they face two big issues due to our own decisions :
                <ul>
                    <li> Training data has poorer and poorer quality. The web has democratized knowledge sharing and the difference on quality is getting wider and wider.
                    </li>
                    <li> Knowledge is hindered by ideologies and safety. History, economics, and natural sciences are framed inside what is considered true and politically correct and/or safe. Hence contributing to a lack of systematic view and understanding of AI.</li>
                </ul>
            </p>
            <p>
                At the moment, the only valuable thing I see for a company willing to use AI is that it can facilitate the differencitation between the two groups of people.
                A valuable employee will know what he does not know and reach a working solution given the constraints (ethical, legal, technical, ...). To do so, chatgpt can be seen as an assistant but the cost of using it can as well increase over time.
                Paid and sponsor content has killed the quality of the sources of internet, and people trust less and less the machine to organize knowledge.
            </p>
            <p>I am kind of nostalgic of the time where we would "Just Ask Google". Before it was a matter of minutes to find a working solution or a good ressource. Knowadays people feel the need to get better references. More curated ressources, recommended by people we trust.
                As the quality of what we find alone will get poorer, we wont ask another machine for help, we will turn directly to humans. Through communities or good old books. I dont even remember how and when my search queries on google went from "how to ..." to "how to ... reddit" 
                Amazon has become a better search engine as you have notation and comments. If you look for a product or knowledge through book, I find it sometimes more straightgorward.
                Now writing the article, I realize that the true power of google was to materialize the trust of humans over knowledge sharing. We decided to trade the human soul hidden into the search engine against faster results :'( .</p>
            </p>
            <p>Chatgpt and other llms will have the same faith as the search engines : making humans more humans and self-resilients.
            </p>
            

            <h3> Managers and annual reviews</h3>

            <p>The mere thought of an annual performance review of the technology by a manager is hilarious. This argument is silly I must admit. Unless you need to evaluate the productivity of your team ... composed by llms.
                Supposing that llm deliver a consistent and reliable output and can replace humans on some tasks. 
                (By the way who are going to break down the problems and add the pieces back to form a comprehensible big program. Ensure safety and get responsability ?)
                Anyway in that scenario, we could witness what has happened in the industrial and agricultural sector, or what happens with automatic cashiers. But the difference is, at least for collaborative and iterative endeavors : how to measure productivity ?
            </p>   
            <p>
                In the software engineering realm, how to measure productivity ? what are the metrics ? the number of developed features or, is the number of bugs resolved, the added traffic to your app and website ? As long as we do not have metrics to evaluate productivity from a micro point of view (in a team and product) in swe, 
                nobody will know if llms improved something? 
            </p>
            <p>Every line of code is a debt and several factors will turn this debt into a profit. Factors that are not digital. Technology is not the answer to that problem. 
                Blindly adding debt is not gonna turn a non profitable startup into the new facebook.
            </p>
            
            
            

        </article>
    </body>
</html>